{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn_crfsuite\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install seqeval\n",
        "!pip install tabulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaAy6lL9ZPTW",
        "outputId": "61a7da33-2554-4b20-9480-a1017a064b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.7 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (0.9.11)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (3.6.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzjtSFPGUK5l"
      },
      "outputs": [],
      "source": [
        "# CRF-based NER model for Telugu WikiAnn data with comprehensive evaluation\n",
        "# --------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics\n",
        "from collections import Counter\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction functions for CRF\n",
        "# ----------------------------------\n",
        "def word2features(sent, i):\n",
        "    \"\"\"Extract features from word at position i.\"\"\"\n",
        "    word = sent[i]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:] if len(word) >= 3 else word,\n",
        "        'word[-2:]': word[-2:] if len(word) >= 2 else word,\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'word.contains_hyphen': '-' in word,\n",
        "        'word.contains_dot': '.' in word,\n",
        "        'word.length': len(word),\n",
        "        'word.prefix-2': word[:2] if len(word) >= 2 else word,\n",
        "        'word.prefix-3': word[:3] if len(word) >= 3 else word,\n",
        "        'word.suffix-2': word[-2:] if len(word) >= 2 else word,\n",
        "        'word.suffix-3': word[-3:] if len(word) >= 3 else word,\n",
        "        'word.contains_digit': any(char.isdigit() for char in word),\n",
        "        'word.contains_uppercase': any(char.isupper() for char in word),\n",
        "        'word.is_alphanumeric': word.isalnum(),\n",
        "        'word.is_alphabetic': word.isalpha(),\n",
        "    }\n",
        "\n",
        "    # Context features - previous word\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:word.isdigit()': word1.isdigit(),\n",
        "            '-1:word.prefix-2': word1[:2] if len(word1) >= 2 else word1,\n",
        "            '-1:word.suffix-2': word1[-2:] if len(word1) >= 2 else word1,\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    # Context features - next word\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:word.isdigit()': word1.isdigit(),\n",
        "            '+1:word.prefix-2': word1[:2] if len(word1) >= 2 else word1,\n",
        "            '+1:word.suffix-2': word1[-2:] if len(word1) >= 2 else word1,\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    # Additional context features\n",
        "    if i > 1:\n",
        "        word2 = sent[i-2]\n",
        "        features.update({\n",
        "            '-2:word.lower()': word2.lower(),\n",
        "            '-2:word.istitle()': word2.istitle(),\n",
        "        })\n",
        "\n",
        "    if i < len(sent)-2:\n",
        "        word2 = sent[i+2]\n",
        "        features.update({\n",
        "            '+2:word.lower()': word2.lower(),\n",
        "            '+2:word.istitle()': word2.istitle(),\n",
        "        })\n",
        "\n",
        "    # Telugu-specific features\n",
        "    features.update({\n",
        "        'word.telugu_chars': sum(1 for c in word if '\\u0C00' <= c <= '\\u0C7F'),\n",
        "        'word.is_telugu': all('\\u0C00' <= c <= '\\u0C7F' for c in word if c.isalpha()),\n",
        "        'word.script_mixed': any('\\u0C00' <= c <= '\\u0C7F' for c in word) and\n",
        "                            any(not('\\u0C00' <= c <= '\\u0C7F') for c in word if c.isalpha())\n",
        "    })\n",
        "\n",
        "    return features\n",
        "\n",
        "def sent2features(sent):\n",
        "    \"\"\"Convert sentence to list of features.\"\"\"\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(tags):\n",
        "    \"\"\"Convert numeric tags to string labels for CRF.\"\"\"\n",
        "    return [str(tag) for tag in tags]  # Convert integers to strings\n"
      ],
      "metadata": {
        "id": "qB_2RwxmaLqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main CRF NER class\n",
        "# -----------------\n",
        "class CRFNER:\n",
        "    def __init__(self, c1=0.1, c2=0.1, max_iterations=100):\n",
        "        self.crf = sklearn_crfsuite.CRF(\n",
        "            algorithm='lbfgs',\n",
        "            c1=c1,\n",
        "            c2=c2,\n",
        "            max_iterations=max_iterations,\n",
        "            all_possible_transitions=True\n",
        "        )\n",
        "        self.allowed_entities = {\"PER\", \"LOC\", \"ORG\"}  # WikiAnn has these three entity types\n",
        "        self.tag_to_label = {\n",
        "            '0': 'O',\n",
        "            '1': 'B-PER', '2': 'I-PER',\n",
        "            '3': 'B-ORG', '4': 'I-ORG',\n",
        "            '5': 'B-LOC', '6': 'I-LOC'\n",
        "        }\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        \"\"\"Train CRF model.\"\"\"\n",
        "        self.crf.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        \"\"\"Predict using CRF model.\"\"\"\n",
        "        return self.crf.predict(X_test)\n",
        "\n",
        "    def get_transition_features(self):\n",
        "        \"\"\"Get learned transition features.\"\"\"\n",
        "        if hasattr(self.crf, 'transition_features_'):\n",
        "            return dict(self.crf.transition_features_)\n",
        "        return {}\n",
        "\n",
        "    def get_state_features(self):\n",
        "        \"\"\"Get learned state features.\"\"\"\n",
        "        if hasattr(self.crf, 'state_features_'):\n",
        "            return dict(self.crf.state_features_)\n",
        "        return {}\n",
        "\n",
        "    def convert_to_iob_tags(self, predictions):\n",
        "        \"\"\"Convert string numeric tags to IOB format.\"\"\"\n",
        "        return [[self.tag_to_label[tag] for tag in seq] for seq in predictions]\n"
      ],
      "metadata": {
        "id": "JyuY61Q11CkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for CRF\n",
        "# -------------------\n",
        "def prepare_crf_data(dataset):\n",
        "    \"\"\"Prepare WikiAnn data for CRF training and testing.\"\"\"\n",
        "    # Prepare training data\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for example in dataset[\"train\"]:\n",
        "        tokens = example[\"tokens\"]\n",
        "        tags = example[\"ner_tags\"]\n",
        "        X_train.append(sent2features(tokens))\n",
        "        y_train.append(sent2labels(tags))\n",
        "\n",
        "    # Prepare test data\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "    for example in dataset[\"test\"]:\n",
        "        tokens = example[\"tokens\"]\n",
        "        tags = example[\"ner_tags\"]\n",
        "        X_test.append(sent2features(tokens))\n",
        "        y_test.append(sent2labels(tags))\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Convert numeric tags to IOB format tags\n",
        "def convert_numeric_to_iob_tags(tag_ids):\n",
        "    \"\"\"Convert numeric tags to IOB format tags.\"\"\"\n",
        "    id_to_tag = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\", 2: \"I-PER\",\n",
        "        3: \"B-ORG\", 4: \"I-ORG\",\n",
        "        5: \"B-LOC\", 6: \"I-LOC\"\n",
        "    }\n",
        "\n",
        "    return [id_to_tag[int(tag_id)] for tag_id in tag_ids]\n"
      ],
      "metadata": {
        "id": "SlSqgRvp1GMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_crf_ner_system(crf_model, X_test, y_test, tag_scheme=\"IOB2\"): # Changed 'iob2' to 'IOB2'\n",
        "    \"\"\"Evaluate CRF NER system using seqeval.\"\"\"\n",
        "    # Get predictions\n",
        "    y_pred = crf_model.predict(X_test)\n",
        "\n",
        "    # Convert string predictions and references to IOB format\n",
        "    iob_predictions = [convert_numeric_to_iob_tags(pred_seq) for pred_seq in y_pred]\n",
        "    iob_references = [convert_numeric_to_iob_tags(ref_seq) for ref_seq in y_test]\n",
        "\n",
        "    # Evaluate using seqeval\n",
        "    metric = load(\"seqeval\")\n",
        "    results = metric.compute(predictions=iob_predictions, references=iob_references, scheme=tag_scheme)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\\033[1;34mCRF NER Evaluation Results:\\033[0m\")\n",
        "\n",
        "    # Create detailed table for per-entity results\n",
        "    crf_table = []\n",
        "    for entity, metrics in results.items():\n",
        "        if isinstance(metrics, dict) and any(e in entity for e in crf_model.allowed_entities):\n",
        "            crf_table.append([\n",
        "                entity,\n",
        "                f\"{metrics['precision']:.4f}\",\n",
        "                f\"{metrics['recall']:.4f}\",\n",
        "                f\"{metrics['f1']:.4f}\",\n",
        "                metrics['number']\n",
        "            ])\n",
        "\n",
        "    print(tabulate(crf_table, headers=[\"Entity\", \"Precision\", \"Recall\", \"F1 Score\", \"Support\"], tablefmt=\"pretty\"))\n",
        "\n",
        "    # Overall results\n",
        "    print(f\"\\n\\033[1;36mOverall Metrics:\\033[0m\")\n",
        "    print(f\"Overall Precision: {results['overall_precision']:.4f}\")\n",
        "    print(f\"Overall Recall: {results['overall_recall']:.4f}\")\n",
        "    print(f\"Overall F1 Score: {results['overall_f1']:.4f}\")\n",
        "    print(f\"Overall Accuracy: {results['overall_accuracy']:.4f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "x1nS6bEQ1Jig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate detailed seqeval report\n",
        "# ------------------------------\n",
        "def generate_crf_seqeval_report(crf_model, X_test, y_test, tag_scheme=\"iob2\"):\n",
        "    \"\"\"Generate detailed seqeval classification report for CRF.\"\"\"\n",
        "    # Get predictions\n",
        "    y_pred = crf_model.predict(X_test)\n",
        "\n",
        "    # Convert string predictions and references to IOB format\n",
        "    iob_predictions = [convert_numeric_to_iob_tags(pred_seq) for pred_seq in y_pred]\n",
        "    iob_references = [convert_numeric_to_iob_tags(ref_seq) for ref_seq in y_test]\n",
        "\n",
        "    # Generate seqeval classification report\n",
        "    metric = load(\"seqeval\")\n",
        "    results = metric.compute(predictions=iob_predictions, references=iob_references, mode='strict', scheme=tag_scheme) # Using the correct tag_scheme\n",
        "    print(\"\\n\\033[1;34mCRF NER Classification Report (seqeval):\\033[0m\")\n",
        "\n",
        "    # Format results into a readable report\n",
        "    print(f\"Overall precision: {results['overall_precision']:.4f}\")\n",
        "    print(f\"Overall recall: {results['overall_recall']:.4f}\")\n",
        "    print(f\"Overall F1: {results['overall_f1']:.4f}\")\n",
        "    print(f\"Overall accuracy: {results['overall_accuracy']:.4f}\")\n",
        "\n",
        "    print(\"\\nEntity-level metrics:\")\n",
        "    for entity in [\"PER\", \"ORG\", \"LOC\"]:\n",
        "        entity_key = entity  # The key in results dict\n",
        "        if entity_key in results:\n",
        "            ent_results = results[entity_key]\n",
        "            print(f\"  {entity}:\")\n",
        "            print(f\"    Precision: {ent_results['precision']:.4f}\")\n",
        "            print(f\"    Recall: {ent_results['recall']:.4f}\")\n",
        "            print(f\"    F1: {ent_results['f1']:.4f}\")\n",
        "            print(f\"    Support: {ent_results['number']}\")\n",
        "\n",
        "    return y_pred, y_test"
      ],
      "metadata": {
        "id": "AX8zz-CG1Lwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze CRF model features\n",
        "# ------------------------\n",
        "def analyze_crf_features(crf_model):\n",
        "    \"\"\"Analyze important features learned by CRF.\"\"\"\n",
        "    # Get state and transition features\n",
        "    state_features = crf_model.get_state_features()\n",
        "    transition_features = crf_model.get_transition_features()\n",
        "\n",
        "    # Mapping from numeric labels to IOB tags for better readability\n",
        "    tag_to_label = {\n",
        "        '0': 'O',\n",
        "        '1': 'B-PER', '2': 'I-PER',\n",
        "        '3': 'B-ORG', '4': 'I-ORG',\n",
        "        '5': 'B-LOC', '6': 'I-LOC'\n",
        "    }\n",
        "\n",
        "    # Top state features for each label\n",
        "    print(\"\\n\\033[1;34mTop State Features by Label:\\033[0m\")\n",
        "    label_features = {}\n",
        "\n",
        "    for (label, feature), weight in state_features.items():\n",
        "        if label not in label_features:\n",
        "            label_features[label] = []\n",
        "        label_features[label].append((feature, weight))\n",
        "\n",
        "    # Sort and display top features for each label\n",
        "    for label, features in label_features.items():\n",
        "        readable_label = tag_to_label.get(label, label)\n",
        "        print(f\"\\n\\033[1;36m{readable_label} (internal: {label}):\\033[0m\")\n",
        "        top_features = sorted(features, key=lambda x: abs(x[1]), reverse=True)[:10]\n",
        "        for feature, weight in top_features:\n",
        "            print(f\"  {feature}: {weight:.4f}\")\n",
        "\n",
        "    # Top transition features\n",
        "    print(\"\\n\\033[1;34mTop Transition Features:\\033[0m\")\n",
        "    top_transitions = sorted(transition_features.items(), key=lambda x: abs(x[1]), reverse=True)[:20]\n",
        "    for (from_label, to_label), weight in top_transitions:\n",
        "        readable_from = tag_to_label.get(from_label, from_label)\n",
        "        readable_to = tag_to_label.get(to_label, to_label)\n",
        "        print(f\"  {readable_from} ({from_label}) -> {readable_to} ({to_label}): {weight:.4f}\")\n"
      ],
      "metadata": {
        "id": "tYS3ftiy1Nk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Telugu sample data exploration function\n",
        "# ----------------------------------------\n",
        "def explore_telugu_dataset(dataset):\n",
        "    \"\"\"Explore the Telugu dataset to better understand its structure.\"\"\"\n",
        "    # Print some statistics\n",
        "    print(\"\\n\\033[1;34mTelugu WikiAnn Dataset Exploration:\\033[0m\")\n",
        "    print(f\"Training set: {len(dataset['train'])} sentences\")\n",
        "    print(f\"Test set: {len(dataset['test'])} sentences\")\n",
        "    print(f\"Validation set: {len(dataset['validation'])} sentences\")\n",
        "\n",
        "    # Count tokens\n",
        "    train_tokens = sum(len(ex['tokens']) for ex in dataset['train'])\n",
        "    test_tokens = sum(len(ex['tokens']) for ex in dataset['test'])\n",
        "    val_tokens = sum(len(ex['tokens']) for ex in dataset['validation'])\n",
        "\n",
        "    print(f\"\\nTotal tokens: {train_tokens + test_tokens + val_tokens}\")\n",
        "    print(f\"Training tokens: {train_tokens}\")\n",
        "    print(f\"Test tokens: {test_tokens}\")\n",
        "    print(f\"Validation tokens: {val_tokens}\")\n",
        "\n",
        "    # Count named entities\n",
        "    tag_mapping = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\", 2: \"I-PER\",\n",
        "        3: \"B-ORG\", 4: \"I-ORG\",\n",
        "        5: \"B-LOC\", 6: \"I-LOC\"\n",
        "    }\n",
        "\n",
        "    entity_counts = {'PER': 0, 'ORG': 0, 'LOC': 0}\n",
        "\n",
        "    # Count entity occurrences (counting B- tags as entity starts)\n",
        "    for example in dataset['train']:\n",
        "        for tag in example['ner_tags']:\n",
        "            if tag == 1:  # B-PER\n",
        "                entity_counts['PER'] += 1\n",
        "            elif tag == 3:  # B-ORG\n",
        "                entity_counts['ORG'] += 1\n",
        "            elif tag == 5:  # B-LOC\n",
        "                entity_counts['LOC'] += 1\n",
        "\n",
        "    print(\"\\nNamed entity counts in training set:\")\n",
        "    for entity, count in entity_counts.items():\n",
        "        print(f\"  {entity}: {count}\")\n",
        "\n",
        "    # Distribution of tag types\n",
        "    tag_counts = Counter()\n",
        "    for example in dataset['train']:\n",
        "        tag_counts.update(example['ner_tags'])\n",
        "\n",
        "    print(\"\\nTag distribution in training set:\")\n",
        "    for tag_id, count in sorted(tag_counts.items()):\n",
        "        print(f\"  {tag_mapping.get(tag_id, tag_id)}: {count}\")\n",
        "\n",
        "    # Print a few examples\n",
        "    print(\"\\nSample examples:\")\n",
        "    for i in range(min(3, len(dataset['train']))):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        tokens = dataset['train'][i]['tokens']\n",
        "        tags = dataset['train'][i]['ner_tags']\n",
        "        print(\"Tokens:\", tokens)\n",
        "        print(\"Tags:\", tags)\n",
        "        print(\"IOB Tags:\", [tag_mapping.get(tag, tag) for tag in tags])\n"
      ],
      "metadata": {
        "id": "7XI5edXw1SFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution function for CRF evaluation\n",
        "# ----------------------------------------\n",
        "def run_crf_evaluation():\n",
        "    \"\"\"Run CRF model training and evaluation.\"\"\"\n",
        "    # Load dataset\n",
        "    print(\"Loading Telugu WikiAnn NER dataset...\")\n",
        "    dataset = load_dataset(\"wikiann\", \"te\")  # Using the lang code 'te' for Telugu\n",
        "\n",
        "    # Explore the dataset\n",
        "    explore_telugu_dataset(dataset)\n",
        "\n",
        "    # Prepare data\n",
        "    print(\"\\nPreparing data for CRF...\")\n",
        "    X_train, y_train, X_test, y_test = prepare_crf_data(dataset)\n",
        "\n",
        "    print(f\"Training examples: {len(X_train)}\")\n",
        "    print(f\"Test examples: {len(X_test)}\")\n",
        "\n",
        "    # Initialize and train CRF model\n",
        "    print(\"Training CRF model...\")\n",
        "    crf_model = CRFNER(c1=0.1, c2=0.1, max_iterations=100)\n",
        "    crf_model.train(X_train, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"\\nEvaluating CRF NER system...\")\n",
        "    results = evaluate_crf_ner_system(crf_model, X_test, y_test, tag_scheme=\"IOB2\")\n",
        "\n",
        "    # Generate detailed classification report\n",
        "    print(\"\\nGenerating detailed classification report...\")\n",
        "    generate_crf_seqeval_report(crf_model, X_test, y_test, tag_scheme=\"IOB2\")\n",
        "\n",
        "\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the CRF evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    run_crf_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8ajYRnj4Zy4",
        "outputId": "a7b14b52-f207-4baa-abff-0e32dbdf965a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Telugu WikiAnn NER dataset...\n",
            "\n",
            "\u001b[1;34mTelugu WikiAnn Dataset Exploration:\u001b[0m\n",
            "Training set: 1000 sentences\n",
            "Test set: 1000 sentences\n",
            "Validation set: 1000 sentences\n",
            "\n",
            "Total tokens: 24322\n",
            "Training tokens: 8087\n",
            "Test tokens: 8155\n",
            "Validation tokens: 8080\n",
            "\n",
            "Named entity counts in training set:\n",
            "  PER: 364\n",
            "  ORG: 347\n",
            "  LOC: 493\n",
            "\n",
            "Tag distribution in training set:\n",
            "  O: 5834\n",
            "  B-PER: 364\n",
            "  I-PER: 302\n",
            "  B-ORG: 347\n",
            "  I-ORG: 568\n",
            "  B-LOC: 493\n",
            "  I-LOC: 179\n",
            "\n",
            "Sample examples:\n",
            "\n",
            "Example 1:\n",
            "Tokens: ['ఆంధ్ర', 'ప్రదేశ్లో', ',', 'వారు', 'జనాభాలో', '2', '%', 'కంటే', 'తక్కువగాను', ',', 'తమిళనాడులో', 'వారు', '3', '%', 'కన్నా', 'తక్కువ', 'మంది', 'ఉన్నారు', '.']\n",
            "Tags: [5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "IOB Tags: ['B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "Example 2:\n",
            "Tokens: ['దక్షిణాఫ్రికాతో', 'కలిసి', 'ఒక', 'పెట్టుబడి', 'కార్యక్రమాన్ని', 'గార్డనర్', 'రూపొందిస్తున్నట్లు', 'చెప్పబడుతుంది', '.']\n",
            "Tags: [5, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "IOB Tags: ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "Example 3:\n",
            "Tokens: ['బెంగుళూరు', 'విశ్వవిద్యాలయ', 'స్నాతకోత్సవ', 'సందర్భంగా', 'ఆయన', 'ఉపన్యాసం', ',', \"''విచారక్రాంతిగే\", 'ఆహ్వాన', \"''\", 'అనే', 'గ్రంధంలో', 'ప్రచురింపబడింది', '.']\n",
            "Tags: [3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "IOB Tags: ['B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "Preparing data for CRF...\n",
            "Training examples: 1000\n",
            "Test examples: 1000\n",
            "Training CRF model...\n",
            "\n",
            "Evaluating CRF NER system...\n",
            "\n",
            "\u001b[1;34mCRF NER Evaluation Results:\u001b[0m\n",
            "+--------+-----------+--------+----------+---------+\n",
            "| Entity | Precision | Recall | F1 Score | Support |\n",
            "+--------+-----------+--------+----------+---------+\n",
            "|  LOC   |  0.7781   | 0.6467 |  0.7063  |   450   |\n",
            "|  ORG   |  0.7790   | 0.6118 |  0.6853  |   340   |\n",
            "|  PER   |  0.7374   | 0.5381 |  0.6222  |   381   |\n",
            "+--------+-----------+--------+----------+---------+\n",
            "\n",
            "\u001b[1;36mOverall Metrics:\u001b[0m\n",
            "Overall Precision: 0.7661\n",
            "Overall Recall: 0.6012\n",
            "Overall F1 Score: 0.6737\n",
            "Overall Accuracy: 0.8964\n",
            "\n",
            "Generating detailed classification report...\n",
            "\n",
            "\u001b[1;34mCRF NER Classification Report (seqeval):\u001b[0m\n",
            "Overall precision: 0.7661\n",
            "Overall recall: 0.6012\n",
            "Overall F1: 0.6737\n",
            "Overall accuracy: 0.8964\n",
            "\n",
            "Entity-level metrics:\n",
            "  PER:\n",
            "    Precision: 0.7374\n",
            "    Recall: 0.5381\n",
            "    F1: 0.6222\n",
            "    Support: 381\n",
            "  ORG:\n",
            "    Precision: 0.7790\n",
            "    Recall: 0.6118\n",
            "    F1: 0.6853\n",
            "    Support: 340\n",
            "  LOC:\n",
            "    Precision: 0.7781\n",
            "    Recall: 0.6467\n",
            "    F1: 0.7063\n",
            "    Support: 450\n"
          ]
        }
      ]
    }
  ]
}