{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shriyatha/Named_Entity_Recognition/blob/main/BERT_ENGLISH_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poZ6x1woyxyD"
      },
      "outputs": [],
      "source": [
        "# BERT-based Named Entity Recognition (NER) on CoNLL-2003 Dataset\n",
        "# =============================================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install transformers datasets evaluate seqeval torch tqdm matplotlib pandas seaborn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RawTwk0zLf0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForTokenClassification,\n",
        "    DataCollatorForTokenClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukzCoEJgzXw-"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9C0UR4azeuS"
      },
      "outputs": [],
      "source": [
        "# 1. Data Loading and Exploration\n",
        "# ==============================\n",
        "print(\"\\n==== Data Loading and Exploration ====\")\n",
        "# Load dataset and metric\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "metric = load(\"seqeval\")\n",
        "\n",
        "# Basic dataset info\n",
        "print(\"\\nDataset splits:\")\n",
        "for split in dataset.keys():\n",
        "    print(f\"- {split}: {dataset[split].num_rows} examples\")\n",
        "\n",
        "# Examine data structure\n",
        "print(\"\\nDataset features:\", dataset[\"train\"].features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueoT49n9zzfR"
      },
      "outputs": [],
      "source": [
        "# 2. Data Preprocessing\n",
        "# ====================\n",
        "print(\"\\n==== Data Preprocessing ====\")\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# Define label_list\n",
        "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "# Preprocess function to align labels with tokens\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)  # Special tokens get -100\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])  # First token of word\n",
        "            else:\n",
        "                label_ids.append(-100)  # Subsequent tokens of word get -100\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "start_time = time.time()\n",
        "# Tokenize dataset\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "tokenization_time = time.time() - start_time\n",
        "print(f\"Tokenization completed in {tokenization_time:.2f} seconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4EboZ15z8xd"
      },
      "outputs": [],
      "source": [
        "# 3. Model Setup\n",
        "# =============\n",
        "print(\"\\n==== Model Setup ====\")\n",
        "# Create data collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 16\n",
        "print(f\"Using batch size: {batch_size}\")\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"test\"],\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "print(\"\\nInitializing BERT model for token classification...\")\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels=len(label_list),\n",
        "    id2label={i: label for i, label in enumerate(label_list)},\n",
        "    label2id={label: i for i, label in enumerate(label_list)}\n",
        ").to(device)\n",
        "\n",
        "# Print model architecture\n",
        "print(\"\\nModel architecture:\")\n",
        "print(model.__class__.__name__)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yf448Ns0Hv5"
      },
      "outputs": [],
      "source": [
        "# 4. Training Setup\n",
        "# ================\n",
        "print(\"\\n==== Training Setup ====\")\n",
        "# Hyperparameters\n",
        "learning_rate = 2e-5\n",
        "epochs = 5\n",
        "weight_decay = 0.01\n",
        "warmup_steps = 0\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Epochs: {epochs}\")\n",
        "print(f\"Weight decay: {weight_decay}\")\n",
        "print(f\"Warmup steps: {warmup_steps}\")\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAGlcNSQ0Orz"
      },
      "outputs": [],
      "source": [
        "# 5. Training Loop\n",
        "# ==============\n",
        "print(\"\\n==== Training Loop ====\")\n",
        "# Helper function for evaluation\n",
        "def evaluate(dataloader, desc=\"Evaluating\"):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    for batch in tqdm(dataloader, desc=desc):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            batch_preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "            batch_labels = batch[\"labels\"].cpu().numpy()\n",
        "        for preds, labels in zip(batch_preds, batch_labels):\n",
        "            # Filter out ignored index (-100)\n",
        "            true_indices = [i for i, l in enumerate(labels) if l != -100]\n",
        "            true_labels.append([label_list[labels[i]] for i in true_indices])\n",
        "            predictions.append([label_list[preds[i]] for i in true_indices])\n",
        "    results = metric.compute(predictions=predictions, references=true_labels)\n",
        "    return results, predictions, true_labels\n",
        "\n",
        "# Store metrics for plotting\n",
        "train_losses = []\n",
        "val_f1_scores = []\n",
        "print(\"\\nStarting training...\")\n",
        "start_training_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    # Training\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    val_results, _, _ = evaluate(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\")\n",
        "    val_f1_scores.append(val_results[\"overall_f1\"])\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs} completed in {epoch_time:.2f} seconds\")\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Validation F1: {val_results['overall_f1']:.4f}\")\n",
        "    print(f\"Validation Precision: {val_results['overall_precision']:.4f}\")\n",
        "    print(f\"Validation Recall: {val_results['overall_recall']:.4f}\")\n",
        "\n",
        "total_training_time = time.time() - start_training_time\n",
        "print(f\"\\nTraining completed in {total_training_time:.2f} seconds ({total_training_time/60:.2f} minutes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fvy5c4U0n2R"
      },
      "outputs": [],
      "source": [
        "# Plot training progress\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, epochs + 1), train_losses, marker='o')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, epochs + 1), val_f1_scores, marker='o', color='green')\n",
        "plt.title(\"Validation F1 Score\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_progress.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccz97Agi0qT1"
      },
      "outputs": [],
      "source": [
        "# 6. Test Evaluation and Analysis\n",
        "# ==============================\n",
        "print(\"\\n==== Test Evaluation and Analysis ====\")\n",
        "print(\"Evaluating on test set...\")\n",
        "test_results, test_predictions, test_true_labels = evaluate(test_dataloader, \"Testing\")\n",
        "\n",
        "# Print overall metrics\n",
        "print(\"\\nTest Results:\")\n",
        "print(f\"Accuracy: {test_results['overall_accuracy']:.4f}\")\n",
        "print(f\"Precision: {test_results['overall_precision']:.4f}\")\n",
        "print(f\"Recall: {test_results['overall_recall']:.4f}\")\n",
        "print(f\"F1 Score: {test_results['overall_f1']:.4f}\")\n",
        "\n",
        "# Print per-entity metrics\n",
        "print(\"\\nPer-Entity Type Metrics:\")\n",
        "entity_results = {}\n",
        "for key in sorted(test_results.keys()):\n",
        "    if key not in ['overall_accuracy', 'overall_precision', 'overall_recall', 'overall_f1']:\n",
        "        entity_results[key] = {\n",
        "            'precision': test_results[key]['precision'],\n",
        "            'recall': test_results[key]['recall'],\n",
        "            'f1': test_results[key]['f1'],\n",
        "            'number': test_results[key]['number']\n",
        "        }\n",
        "        print(f\"{key}:\")\n",
        "        print(f\" Precision: {test_results[key]['precision']:.4f}\")\n",
        "        print(f\" Recall: {test_results[key]['recall']:.4f}\")\n",
        "        print(f\" F1: {test_results[key]['f1']:.4f}\")\n",
        "        print(f\" Support: {test_results[key]['number']} entities\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPlXDBxpF98lLtduPXZNNWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}