{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shriyatha/Named_Entity_Recognition/blob/main/DATASET_ANALYSIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKF3Jhmtu9wo"
      },
      "source": [
        "#NLP PROJECT: NAMED ENTITY RECOGNITION ON CONLL2003 DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2DMFdQLCS7s"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers seqeval evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAXvgLPKvKZX"
      },
      "source": [
        "# NOW LOADING THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsYejCXoEJ1P"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "# Get the label names\n",
        "label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "# Function to map numerical tags to feature names\n",
        "def map_tags_to_names(tags, label_names):\n",
        "    return [label_names[tag] for tag in tags]\n",
        "\n",
        "# Example: Map tags for the first sample in the training set\n",
        "ner_tags_numerical = dataset[\"train\"][\"ner_tags\"][0]\n",
        "ner_tags_features = map_tags_to_names(ner_tags_numerical, label_names)\n",
        "\n",
        "# Print tokens and corresponding NER tags\n",
        "tokens = dataset[\"train\"][\"tokens\"][0]\n",
        "for token, tag in zip(tokens, ner_tags_features):\n",
        "    print(f\"{token}: {tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GQhGYjTy2PG"
      },
      "source": [
        "# ANALYSING THE DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3Wwq4QEy6U9"
      },
      "source": [
        "### Calculates and plots the length distribution of entities (e.g., \"PER\", \"ORG\") in an NER dataset,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ft4HN2XEuGF"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize a dictionary to store entity lengths\n",
        "entity_lengths = defaultdict(list)\n",
        "\n",
        "# Iterate through the dataset\n",
        "for tags in dataset[\"train\"][\"ner_tags\"]:\n",
        "    current_entity = None\n",
        "    current_length = 0\n",
        "    for tag in tags:\n",
        "        if tag == 0:  # \"O\" tag\n",
        "            if current_entity:\n",
        "                entity_lengths[current_entity].append(current_length)\n",
        "                current_entity = None\n",
        "                current_length = 0\n",
        "        else:\n",
        "            tag_name = label_names[tag]\n",
        "            if tag_name.startswith(\"B-\"):\n",
        "                if current_entity:\n",
        "                    entity_lengths[current_entity].append(current_length)\n",
        "                current_entity = tag_name[2:]  # Remove \"B-\" or \"I-\"\n",
        "                current_length = 1\n",
        "            elif tag_name.startswith(\"I-\"):\n",
        "                current_length += 1\n",
        "    if current_entity:\n",
        "        entity_lengths[current_entity].append(current_length)\n",
        "\n",
        "# Plot the distribution of entity lengths\n",
        "for entity, lengths in entity_lengths.items():\n",
        "    plt.hist(lengths, bins=range(1, max(lengths) + 1), alpha=0.5, label=entity)\n",
        "plt.xlabel(\"Entity Length\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Length Distribution of Entities\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE_XE1CEFooF"
      },
      "source": [
        "# Most Frequent Entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSrsFV_QFfV1"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flatten the list of NER tags and map to feature names\n",
        "all_tags = [tag for sublist in dataset[\"train\"][\"ner_tags\"] for tag in sublist]\n",
        "all_tags_features = map_tags_to_names(all_tags, label_names)\n",
        "\n",
        "# Filter out \"O\" tags and count entity frequencies (ignoring B- and I- prefixes)\n",
        "entity_frequencies = Counter(tag[2:] for tag in all_tags_features if tag != \"O\" and tag[2:] in {\"PER\", \"LOC\", \"ORG\", \"MISC\"})\n",
        "\n",
        "# Print most frequent entities\n",
        "print(\"Most frequent entities:\")\n",
        "print(entity_frequencies.most_common())\n",
        "\n",
        "# Prepare data for plotting\n",
        "entities = list(entity_frequencies.keys())\n",
        "counts = list(entity_frequencies.values())\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(entities, counts, color=['blue', 'green', 'red', 'purple'])\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Entity Types')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency Distribution of Entity Types in CoNLL Dataset')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-zIa6x8FnlL"
      },
      "source": [
        "# Co-occurrence of entity pairs in sentences from a CoNLL-formatted dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMYCji9FFifz"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize a dictionary to store co-occurrence counts\n",
        "co_occurrence = defaultdict(int)\n",
        "\n",
        "# Iterate through the dataset\n",
        "for tags in dataset[\"train\"][\"ner_tags\"]:\n",
        "    # Get unique entities in the sentence\n",
        "    entities_in_sentence = set()\n",
        "    for tag in tags:\n",
        "        if tag != 0:  # Ignore \"O\" tags\n",
        "            entity = label_names[tag][2:]  # Remove \"B-\" or \"I-\"\n",
        "            if entity in {\"PER\", \"LOC\", \"ORG\", \"MISC\"}:  # Filter specific entity types\n",
        "                entities_in_sentence.add(entity)\n",
        "    # Update co-occurrence counts\n",
        "    entities_in_sentence = list(entities_in_sentence)\n",
        "    for i in range(len(entities_in_sentence)):\n",
        "        for j in range(i + 1, len(entities_in_sentence)):\n",
        "            pair = tuple(sorted([entities_in_sentence[i], entities_in_sentence[j]]))\n",
        "            co_occurrence[pair] += 1\n",
        "\n",
        "# Print the most frequent co-occurring entity pairs\n",
        "print(\"Most frequent co-occurring entity pairs:\")\n",
        "for pair, count in sorted(co_occurrence.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "    print(f\"{pair}: {count}\")\n",
        "\n",
        "# Visualization: Co-occurrence heatmap\n",
        "co_occurrence_df = pd.DataFrame(list(co_occurrence.items()), columns=[\"Pair\", \"Count\"])\n",
        "co_occurrence_df[[\"Entity1\", \"Entity2\"]] = pd.DataFrame(co_occurrence_df[\"Pair\"].tolist(), index=co_occurrence_df.index)\n",
        "co_occurrence_matrix = co_occurrence_df.pivot(index=\"Entity1\", columns=\"Entity2\", values=\"Count\").fillna(0)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(co_occurrence_matrix, annot=True, fmt=\"f\", cmap=\"YlGnBu\")\n",
        "plt.title(\"Co-occurrence of Entity Pairs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbEw1Ru7FysE"
      },
      "source": [
        "# Context Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZyZN0ZkFwq7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords (if not already downloaded)\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Initialize stopwords set\n",
        "stopwords_set = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Extract words surrounding \"ORG\" entities\n",
        "context_window = 2  # Number of words to consider on each side\n",
        "org_contexts = []\n",
        "\n",
        "for tokens, tags in zip(dataset[\"train\"][\"tokens\"], dataset[\"train\"][\"ner_tags\"]):\n",
        "    tags_features = map_tags_to_names(tags, label_names)\n",
        "    i = 0\n",
        "    while i < len(tags_features):\n",
        "        if tags_features[i] == \"B-ORG\":\n",
        "            # Find the end of the ORG entity\n",
        "            j = i\n",
        "            while j + 1 < len(tags_features) and tags_features[j + 1] == \"I-ORG\":\n",
        "                j += 1\n",
        "            # Extract context around the entire entity\n",
        "            start = max(0, i - context_window)\n",
        "            end = min(len(tokens), j + context_window + 1)\n",
        "            context = [word for word in tokens[start:end] if word.lower() not in stopwords_set and word.isalnum()]\n",
        "            org_contexts.append(context)\n",
        "            i = j + 1  # Skip the rest of the ORG entity\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "# Print the first 10 contexts\n",
        "for context in org_contexts[:10]:\n",
        "    print(\" \".join(context))\n",
        "\n",
        "# Save contexts to a file\n",
        "with open(\"org_contexts.txt\", \"w\") as f:\n",
        "    for context in org_contexts:\n",
        "        f.write(\" \".join(context) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DqBPSGNGCNR"
      },
      "source": [
        "# Find the most common words surrounding a specific entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAzGkMU2F6lg"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Custom stopwords to exclude\n",
        "custom_stopwords = {\"said\", \"told\", \"reuters\", \"newsroom\", \"st\", \"inc\"}\n",
        "\n",
        "# Flatten the list of contexts and convert to lowercase\n",
        "all_context_words = [word.lower() for context in org_contexts for word in context]\n",
        "\n",
        "# Filter out numbers and custom stopwords\n",
        "filtered_words = [word for word in all_context_words if word.isalpha() and word not in custom_stopwords]\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_frequencies = Counter(filtered_words)\n",
        "\n",
        "# Print the 20 most common words in the context of ORG entities\n",
        "print(\"Most common words in the context of ORG entities:\")\n",
        "for word, count in word_frequencies.most_common(20):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Visualization: Bar plot of the most common words\n",
        "top_words, top_counts = zip(*word_frequencies.most_common(20))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(top_words, top_counts, color='skyblue')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Most Common Words in the Context of ORG Entities')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6RrJVkUGGMN"
      },
      "source": [
        "# Entity bound analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdt86f3fF9n7"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize boundary counts\n",
        "boundary_counts = defaultdict(lambda: {\"B\": 0, \"I\": 0})\n",
        "\n",
        "# Iterate through the dataset\n",
        "for tags in dataset[\"train\"][\"ner_tags\"]:\n",
        "    for tag in tags:\n",
        "        if tag != 0:  # Ignore \"O\" tags\n",
        "            tag_name = label_names[tag]\n",
        "            entity_type = tag_name[2:]  # Remove \"B-\" or \"I-\"\n",
        "            boundary = tag_name[0]  # \"B\" or \"I\"\n",
        "            if entity_type in {\"PER\", \"ORG\", \"LOC\", \"MISC\"}:  # Filter specific entity types\n",
        "                boundary_counts[entity_type][boundary] += 1\n",
        "\n",
        "# Print boundary counts\n",
        "for entity, counts in boundary_counts.items():\n",
        "    print(f\"{entity}: B={counts['B']}, I={counts['I']}\")\n",
        "\n",
        "# Visualization: Bar plot of boundary counts\n",
        "entities = list(boundary_counts.keys())\n",
        "b_counts = [boundary_counts[entity][\"B\"] for entity in entities]\n",
        "i_counts = [boundary_counts[entity][\"I\"] for entity in entities]\n",
        "\n",
        "x = range(len(entities))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(x, b_counts, width=0.4, label=\"B\", color=\"blue\", align=\"center\")\n",
        "plt.bar(x, i_counts, width=0.4, label=\"I\", color=\"orange\", align=\"edge\")\n",
        "plt.xlabel('Entity Types')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Counts of \"B\" and \"I\" Tags for Each Entity Type')\n",
        "plt.xticks(x, entities)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS6q7KuB5ZMf"
      },
      "source": [
        "### TOKEN LEVEL ANALYSIS: most common tokens for each entity type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5mCcXhy5SR8"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))  # Load common stopwords\n",
        "ignore_list = {\"new\", \"open\", \"league\", \"cup\", \"party\", \"st\", \"united\"}  # Manually add entity-like common words\n",
        "\n",
        "# Initialize a dictionary to store tokens for each entity type\n",
        "entity_tokens = defaultdict(list)\n",
        "\n",
        "# Extract tokens per entity type\n",
        "for tokens, tags in zip(dataset[\"train\"][\"tokens\"], dataset[\"train\"][\"ner_tags\"]):\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        if 0 <= tag < len(label_names) and label_names[tag] != \"O\":  # Ensure valid tag\n",
        "            entity = label_names[tag][2:]  # Remove \"B-\" or \"I-\"\n",
        "            token = token.lower().strip()  # Normalize token\n",
        "\n",
        "            if token not in stop_words and token not in ignore_list:  # Filter out common words\n",
        "                entity_tokens[entity].append(token)\n",
        "\n",
        "# Print the most common tokens for each entity type\n",
        "for entity, tokens in entity_tokens.items():\n",
        "    print(f\"Most common tokens for {entity}:\")\n",
        "    print(Counter(tokens).most_common(10))\n",
        "\n",
        "# Add the most common tokens to custom entities\n",
        "custom_entities = defaultdict(set)\n",
        "\n",
        "for entity, tokens in entity_tokens.items():\n",
        "    most_common_tokens = Counter(tokens).most_common(10)  # Get top 10 tokens\n",
        "    for token, _ in most_common_tokens:\n",
        "        custom_entities[token].add(entity)  # Store as a set to handle multi-label cases\n",
        "\n",
        "# Print updated custom entities\n",
        "print(\"\\nUpdated Custom Entities:\")\n",
        "for token, entities in custom_entities.items():\n",
        "    print(f\"{token}: {', '.join(entities)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc1C1rrK6FKs"
      },
      "source": [
        "### Sentence Length Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEtM55zL6I5e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate sentence lengths\n",
        "sentence_lengths = [len(tokens) for tokens in dataset[\"train\"][\"tokens\"]]\n",
        "\n",
        "# Plot the distribution\n",
        "plt.hist(sentence_lengths, bins=50)\n",
        "plt.xlabel(\"Sentence Length\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Sentence Lengths\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jai6T4SU5uQh"
      },
      "source": [
        "### Out-of-Vocabulary (OOV) Analysis\n",
        "\n",
        "###### Out-of-vocabulary (OOV) tokens, meaning they are rare words that the model might struggle to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kE0s2G05j-i"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of tokens\n",
        "all_tokens = [token for sublist in dataset[\"train\"][\"tokens\"] for token in sublist]\n",
        "\n",
        "# Count token frequencies\n",
        "token_frequencies = Counter(all_tokens)\n",
        "\n",
        "# Identify low-frequency tokens (potential OOV tokens)\n",
        "oov_tokens = [token for token, count in token_frequencies.items() if count <= 1]\n",
        "print(f\"Number of OOV tokens: {len(oov_tokens)}\")\n",
        "print(f\"Example OOV tokens: {oov_tokens[:30]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcGPtN09VvNV"
      },
      "outputs": [],
      "source": [
        "# Define vocabulary threshold (words appearing more than once are kept)\n",
        "vocab_threshold = 1\n",
        "\n",
        "# Create vocabulary with words above the threshold\n",
        "vocab = {word for word, count in token_frequencies.items() if count > vocab_threshold}\n",
        "\n",
        "# Replace OOV tokens in dataset with [UNK]\n",
        "updated_tokens = [\n",
        "    [token if token in vocab else \"[UNK]\" for token in sublist]\n",
        "    for sublist in dataset[\"train\"][\"tokens\"]\n",
        "]\n",
        "\n",
        "# Print sample\n",
        "print(updated_tokens[:4])  # Show first 3 tokenized sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL-RfDGa51xf"
      },
      "source": [
        "# Dataset split analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMlZouyD5_jP"
      },
      "outputs": [],
      "source": [
        "# Compare entity frequencies across splits\n",
        "for split in [\"train\", \"validation\", \"test\"]:\n",
        "    all_tags = [tag for sublist in dataset[split][\"ner_tags\"] for tag in sublist]\n",
        "    all_entities = [label_names[tag][2:] for tag in all_tags if tag != 0]\n",
        "    entity_frequencies = Counter(all_entities)\n",
        "    print(f\"Entity frequencies in {split} split:\")\n",
        "    print(entity_frequencies.most_common())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}