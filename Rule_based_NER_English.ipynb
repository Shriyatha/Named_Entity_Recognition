{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Enhanced SpaCy NER System for CoNLL2003\n",
        "\n",
        "# 1. Install and Import Required Libraries\n",
        "# ----------------------------------------\n",
        "!pip install -q datasets evaluate tabulate spacy seqeval\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "from tabulate import tabulate\n",
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKGi5j9JQ_fq",
        "outputId": "0e57c13d-0c54-4882-bb16-403c66a28695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m128.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD_hS3vcQmCk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Enhanced Analysis Tools for SpaCy NER System\n",
        "# --------------------------------------------\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "from tabulate import tabulate\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# 1. Add detailed analysis methods to the EnhancedSpacyNER class\n",
        "# ----------------------------------------------------------\n",
        "class EnhancedSpacyNER:\n",
        "    def __init__(self):\n",
        "        # [Previous initialization code remains the same...]\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        # Initialize dictionaries\n",
        "        self.allowed_entities = {\"PER\", \"LOC\", \"ORG\", \"MISC\"}\n",
        "        self.spacy_to_conll = {\n",
        "            \"PERSON\": \"PER\",\n",
        "            \"GPE\": \"LOC\",\n",
        "            \"LOC\": \"LOC\",\n",
        "            \"ORG\": \"ORG\",\n",
        "            \"NORP\": \"MISC\",\n",
        "            \"EVENT\": \"MISC\",\n",
        "            \"PRODUCT\": \"MISC\",\n",
        "            \"WORK_OF_ART\": \"MISC\"\n",
        "        }\n",
        "\n",
        "\n",
        "        self.correction_patterns = {\n",
        "            \"PER\": {\n",
        "                \"title_patterns\": [\n",
        "                    re.compile(r\"^(?:Mr\\.|Mrs\\.|Ms\\.|Dr\\.|Prof\\.|Sir|Lord|Lady) [A-Z][a-z]+(?: [A-Z][a-z]+)*$\"),\n",
        "                ],\n",
        "                \"suffix_patterns\": [\n",
        "                    re.compile(r\"^[A-Z][a-z]+(?: [A-Z][a-z]+)* (?:Jr\\.|Sr\\.|III|II|IV)$\"),\n",
        "                ]\n",
        "            },\n",
        "            \"ORG\": {\n",
        "                \"company_patterns\": [\n",
        "                    re.compile(r\"^[A-Z][a-z]+(?: [A-Z][a-z]+)* (?:Inc|Corp|Co|Ltd|LLC|Limited|PLC|Group|Bank)\\.?$\"),\n",
        "                    re.compile(r\"^The [A-Z][a-z]+(?: [A-Z][a-z]+)* (?:Inc|Corp|Co|Ltd|LLC|Limited|PLC|Group|Bank)\\.?$\"),\n",
        "                ],\n",
        "                \"educational_patterns\": [\n",
        "                    re.compile(r\"^(?:University|College|School|Institute) of [A-Z][a-z]+(?: [A-Z][a-z]+)*$\"),\n",
        "                ]\n",
        "            },\n",
        "            \"LOC\": {\n",
        "                \"geographic_patterns\": [\n",
        "                    re.compile(r\"^[A-Z][a-z]+(?: [A-Z][a-z]+)* (?:Mountains|River|Valley|Desert|Lake|Ocean|Sea|Bay|Gulf|Island)$\"),\n",
        "                    re.compile(r\"^(?:Mount|Mt\\.) [A-Z][a-z]+$\"),\n",
        "                ],\n",
        "                \"admin_patterns\": [\n",
        "                    re.compile(r\"^[A-Z][a-z]+(?: [A-Z][a-z]+)* (?:State|Province|Prefecture|County|District)$\"),\n",
        "                ]\n",
        "            },\n",
        "            \"MISC\": {\n",
        "                \"event_patterns\": [\n",
        "                    re.compile(r\"^[A-Z][a-z]+(?: [A-Z][a-z]+)* (?:Cup|Championship|Olympics|Games|Tournament|Festival|Summit)$\"),\n",
        "                ],\n",
        "                \"award_patterns\": [\n",
        "                    re.compile(r\"^[A-Z][a-z]+(?: [A-Z][a-z]+)* (?:Award|Prize|Medal)$\"),\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.misclassification_corrections = defaultdict(list)\n",
        "\n",
        "    def load_correction_patterns_from_training(self, dataset):\n",
        "        \"\"\"Learn common spaCy misclassifications from training data\"\"\"\n",
        "        id_to_label = dataset[\"train\"].features[\"ner_tags\"].feature.int2str\n",
        "\n",
        "        spacy_errors = defaultdict(Counter)\n",
        "\n",
        "        for tokens, tags in zip(dataset[\"train\"][\"tokens\"], dataset[\"train\"][\"ner_tags\"]):\n",
        "            text = \" \".join(tokens)\n",
        "            doc = self.nlp(text)\n",
        "\n",
        "            spacy_labels = [\"O\"] * len(tokens)\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in self.spacy_to_conll:\n",
        "                    label = self.spacy_to_conll[ent.label_]\n",
        "                    ent_tokens = ent.text.split()\n",
        "                    span = self.find_token_span_exact(tokens, ent_tokens)\n",
        "                    if span:\n",
        "                        start, end = span\n",
        "                        spacy_labels[start] = f\"B-{label}\"\n",
        "                        for i in range(start + 1, end):\n",
        "                            spacy_labels[i] = f\"I-{label}\"\n",
        "\n",
        "            for i, (token, true_tag) in enumerate(zip(tokens, tags)):\n",
        "                true_label = id_to_label(true_tag)\n",
        "                spacy_label = spacy_labels[i]\n",
        "\n",
        "                if true_label != \"O\" and spacy_label != true_label:\n",
        "                    if spacy_label == \"O\":\n",
        "                        entity_type = true_label[2:]\n",
        "                        start_idx = i\n",
        "                        while start_idx > 0 and id_to_label(tags[start_idx-1])[2:] == entity_type:\n",
        "                            start_idx -= 1\n",
        "                        end_idx = i + 1\n",
        "                        while end_idx < len(tags) and id_to_label(tags[end_idx])[2:] == entity_type:\n",
        "                            end_idx += 1\n",
        "                        entity_phrase = \" \".join(tokens[start_idx:end_idx])\n",
        "                        spacy_errors[entity_type][entity_phrase] += 1\n",
        "\n",
        "        for entity_type, error_counter in spacy_errors.items():\n",
        "            for phrase, count in error_counter.most_common(100):\n",
        "                if count > 2:\n",
        "                    escaped_phrase = re.escape(phrase)\n",
        "                    pattern = re.compile(r'\\b' + escaped_phrase + r'\\b')\n",
        "                    self.misclassification_corrections[entity_type].append(pattern)\n",
        "\n",
        "    def find_token_span_exact(self, tokens, target_tokens):\n",
        "        \"\"\"Find exact token span\"\"\"\n",
        "        for i in range(len(tokens) - len(target_tokens) + 1):\n",
        "            if tokens[i:i + len(target_tokens)] == target_tokens:\n",
        "                return i, i + len(target_tokens)\n",
        "        return None\n",
        "\n",
        "    def apply_regex_patterns(self, tokens):\n",
        "        \"\"\"Apply regex pattern-based entity detection\"\"\"\n",
        "        regex_labels = [\"O\"] * len(tokens)\n",
        "\n",
        "        for entity_type, pattern_groups in self.correction_patterns.items():\n",
        "            for group_name, patterns in pattern_groups.items():\n",
        "                for pattern in patterns:\n",
        "                    # Check single tokens\n",
        "                    for i, token in enumerate(tokens):\n",
        "                        if pattern.match(token):\n",
        "                            regex_labels[i] = f\"B-{entity_type}\"\n",
        "\n",
        "                    # Check multi-token phrases (up to 5 tokens)\n",
        "                    for window_size in range(2, 6):\n",
        "                        for i in range(len(tokens) - window_size + 1):\n",
        "                            phrase = \" \".join(tokens[i:i+window_size])\n",
        "                            if pattern.match(phrase):\n",
        "                                regex_labels[i] = f\"B-{entity_type}\"\n",
        "                                for j in range(i+1, i+window_size):\n",
        "                                    regex_labels[j] = f\"I-{entity_type}\"\n",
        "\n",
        "        return regex_labels\n",
        "\n",
        "    def apply_learned_corrections(self, tokens, spacy_labels):\n",
        "        \"\"\"Apply corrections based on learned misclassifications\"\"\"\n",
        "        custom_labels = spacy_labels.copy()\n",
        "\n",
        "        for entity_type, patterns in self.misclassification_corrections.items():\n",
        "            for pattern in patterns:\n",
        "                text = \" \".join(tokens)\n",
        "                for match in pattern.finditer(text):\n",
        "                    matched_text = match.group()\n",
        "                    matched_tokens = matched_text.split()\n",
        "                    span = self.find_token_span_exact(tokens, matched_tokens)\n",
        "\n",
        "                    if span:\n",
        "                        start, end = span\n",
        "                        custom_labels[start] = f\"B-{entity_type}\"\n",
        "                        for i in range(start + 1, end):\n",
        "                            custom_labels[i] = f\"I-{entity_type}\"\n",
        "\n",
        "        return custom_labels\n",
        "\n",
        "    def apply_high_confidence_corrections(self, tokens, spacy_labels):\n",
        "        \"\"\"Apply only high-confidence corrections to spaCy predictions\"\"\"\n",
        "        corrected_labels = spacy_labels.copy()\n",
        "\n",
        "        # Apply pattern-based corrections\n",
        "        for entity_type, pattern_groups in self.correction_patterns.items():\n",
        "            for group_name, patterns in pattern_groups.items():\n",
        "                for pattern in patterns:\n",
        "                    # Check single tokens\n",
        "                    for i, token in enumerate(tokens):\n",
        "                        if pattern.match(token) and spacy_labels[i] != f\"B-{entity_type}\":\n",
        "                            corrected_labels[i] = f\"B-{entity_type}\"\n",
        "\n",
        "                    # Check multi-token phrases (up to 5 tokens)\n",
        "                    for window_size in range(2, 6):\n",
        "                        for i in range(len(tokens) - window_size + 1):\n",
        "                            phrase = \" \".join(tokens[i:i+window_size])\n",
        "                            if pattern.match(phrase):\n",
        "                                # Only correct if spaCy didn't detect it or misclassified it\n",
        "                                if spacy_labels[i] == \"O\" or spacy_labels[i][2:] != entity_type:\n",
        "                                    corrected_labels[i] = f\"B-{entity_type}\"\n",
        "                                    for j in range(i+1, i+window_size):\n",
        "                                        corrected_labels[j] = f\"I-{entity_type}\"\n",
        "\n",
        "        # Apply corrections based on learned misclassifications\n",
        "        for entity_type, patterns in self.misclassification_corrections.items():\n",
        "            for pattern in patterns:\n",
        "                text = \" \".join(tokens)\n",
        "                for match in pattern.finditer(text):\n",
        "                    matched_text = match.group()\n",
        "                    matched_tokens = matched_text.split()\n",
        "                    span = self.find_token_span_exact(tokens, matched_tokens)\n",
        "\n",
        "                    if span:\n",
        "                        start, end = span\n",
        "                        # Only correct if spaCy missed it or got it wrong\n",
        "                        if spacy_labels[start] == \"O\" or spacy_labels[start][2:] != entity_type:\n",
        "                            corrected_labels[start] = f\"B-{entity_type}\"\n",
        "                            for i in range(start + 1, end):\n",
        "                                corrected_labels[i] = f\"I-{entity_type}\"\n",
        "\n",
        "        return corrected_labels\n",
        "\n",
        "    def extract_entities_with_analysis(self, tokens):\n",
        "        \"\"\"Extract entities with multiple methods for analysis\"\"\"\n",
        "        text = \" \".join(tokens)\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Get spaCy predictions\n",
        "        spacy_labels = [\"O\"] * len(tokens)\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in self.spacy_to_conll:\n",
        "                label = self.spacy_to_conll[ent.label_]\n",
        "                if label in self.allowed_entities:\n",
        "                    ent_tokens = ent.text.split()\n",
        "                    span = self.find_token_span_exact(tokens, ent_tokens)\n",
        "                    if span:\n",
        "                        start, end = span\n",
        "                        spacy_labels[start] = f\"B-{label}\"\n",
        "                        for i in range(start + 1, end):\n",
        "                            spacy_labels[i] = f\"I-{label}\"\n",
        "\n",
        "        # Apply pattern-based detection\n",
        "        regex_labels = self.apply_regex_patterns(tokens)\n",
        "\n",
        "        # Apply learned corrections\n",
        "        custom_labels = self.apply_learned_corrections(tokens, spacy_labels)\n",
        "\n",
        "        # Apply high-confidence corrections\n",
        "        corrected_labels = self.apply_high_confidence_corrections(tokens, spacy_labels)\n",
        "\n",
        "        return {\n",
        "            \"spacy\": spacy_labels,\n",
        "            \"regex\": regex_labels,\n",
        "            \"custom\": custom_labels,\n",
        "            \"corrected\": corrected_labels\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Analysis function that compares all approaches\n",
        "# ------------------------------------------------\n",
        "def analyze_examples(dataset, ner_system, max_examples=30):\n",
        "    \"\"\"Analyze examples showing differences between approaches\"\"\"\n",
        "    test_results = []\n",
        "\n",
        "    # Process test data\n",
        "    for example in dataset[\"test\"]:\n",
        "        tokens = example[\"tokens\"]\n",
        "\n",
        "        # Get predictions from all methods\n",
        "        predictions = ner_system.extract_entities_with_analysis(tokens)\n",
        "\n",
        "        # Convert ground truth tags to BIO format\n",
        "        ner_labels = dataset[\"train\"].features[\"ner_tags\"].feature\n",
        "        ground_truth = [ner_labels.int2str(tag) for tag in example[\"ner_tags\"]]\n",
        "        filtered_truth = [tag if any(e in tag for e in ner_system.allowed_entities) else \"O\" for tag in ground_truth]\n",
        "\n",
        "        test_results.append({\n",
        "            \"tokens\": tokens,\n",
        "            \"ground_truth\": filtered_truth,\n",
        "            \"spacy_pred\": predictions[\"spacy\"],\n",
        "            \"regex_pred\": predictions[\"regex\"],\n",
        "            \"custom_pred\": predictions[\"custom\"],\n",
        "            \"corrected_pred\": predictions[\"corrected\"]\n",
        "        })\n",
        "\n",
        "    # Display analysis\n",
        "    count = 0\n",
        "    for i, example in enumerate(test_results):\n",
        "        tokens = example[\"tokens\"]\n",
        "        ground_truth = example[\"ground_truth\"]\n",
        "        spacy_pred = example[\"spacy_pred\"]\n",
        "        custom_pred = example[\"custom_pred\"]\n",
        "        regex_pred = example[\"regex_pred\"]\n",
        "        corrected_pred = example[\"corrected_pred\"]\n",
        "\n",
        "        if ground_truth != corrected_pred or ground_truth != spacy_pred:\n",
        "            print(f\"\\n\\033[1;34mExample {i + 1}:\\033[0m\")  # Blue for example header\n",
        "            print(\"\\033[1;33mTokens: \\033[0m\", \" \".join(tokens))  # Yellow for tokens\n",
        "\n",
        "            # Display differences with aligned columns for readability\n",
        "            print(\"\\033[1;32mGround Truth:    \\033[0m\", \" \".join(ground_truth))\n",
        "            print(\"\\033[1;31mSpaCy Prediction: \\033[0m\", \" \".join(spacy_pred))\n",
        "            print(\"\\033[1;36mRegex Patterns:   \\033[0m\", \" \".join(regex_pred))\n",
        "            print(\"\\033[1;35mLearned Patterns: \\033[0m\", \" \".join(custom_pred))\n",
        "            print(\"\\033[1;36mFinal Corrected:  \\033[0m\", \" \".join(corrected_pred))\n",
        "\n",
        "            # Highlight mismatched entities\n",
        "            mismatched = [\n",
        "                f\"{token}: GT={gt}, SpaCy={sp}, Regex={rp}, Learned={cp}, Final={fp}\"\n",
        "                for token, gt, sp, rp, cp, fp in zip(tokens, ground_truth, spacy_pred, regex_pred, custom_pred, corrected_pred)\n",
        "                if gt != fp or gt != sp\n",
        "            ]\n",
        "\n",
        "            if mismatched:\n",
        "                print(\"\\n\\033[1;35mMismatched Entities:\\033[0m\")\n",
        "                print(\"\\n\".join(mismatched))\n",
        "\n",
        "            print(\"-\" * 100)  # Separator for readability\n",
        "            count += 1\n",
        "\n",
        "            if count >= max_examples:\n",
        "                break\n",
        "\n",
        "    return test_results"
      ],
      "metadata": {
        "id": "eG-hquKBSZuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Generate statistical analysis\n",
        "# -------------------------------\n",
        "def generate_statistics(test_results):\n",
        "    \"\"\"Generate detailed statistics on correction patterns\"\"\"\n",
        "    stats = {\n",
        "        \"spacy_errors\": defaultdict(int),\n",
        "        \"corrections\": defaultdict(int),\n",
        "        \"regex_improvements\": defaultdict(int),\n",
        "        \"learned_improvements\": defaultdict(int),\n",
        "        \"total_corrections\": 0,\n",
        "        \"total_tokens\": 0\n",
        "    }\n",
        "\n",
        "    for example in test_results:\n",
        "        tokens = example[\"tokens\"]\n",
        "        ground_truth = example[\"ground_truth\"]\n",
        "        spacy_pred = example[\"spacy_pred\"]\n",
        "        regex_pred = example[\"regex_pred\"]\n",
        "        custom_pred = example[\"custom_pred\"]\n",
        "        corrected_pred = example[\"corrected_pred\"]\n",
        "\n",
        "        stats[\"total_tokens\"] += len(tokens)\n",
        "\n",
        "        for i, (token, gt, sp, rp, cp, fp) in enumerate(zip(tokens, ground_truth, spacy_pred, regex_pred, custom_pred, corrected_pred)):\n",
        "            if gt != sp:\n",
        "                stats[\"spacy_errors\"][gt] += 1\n",
        "\n",
        "                if fp == gt and sp != gt:\n",
        "                    stats[\"corrections\"][gt] += 1\n",
        "                    stats[\"total_corrections\"] += 1\n",
        "\n",
        "                # Track which method actually helped\n",
        "                if rp == gt and sp != gt:\n",
        "                    stats[\"regex_improvements\"][gt] += 1\n",
        "\n",
        "                if cp == gt and sp != gt:\n",
        "                    stats[\"learned_improvements\"][gt] += 1\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "NqDuK6W6Smoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "def display_analytics(stats):\n",
        "    \"\"\"Display detailed analytics\"\"\"\n",
        "    print(\"\\n\\033[1;34mAnalysis Summary:\\033[0m\")\n",
        "    print(f\"Total tokens analyzed: {stats['total_tokens']}\")\n",
        "    print(f\"Total corrections made: {stats['total_corrections']}\")\n",
        "    print(f\"Correction rate: {stats['total_corrections'] / stats['total_tokens'] * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n\\033[1;34mError Distribution (spaCy):\\033[0m\")\n",
        "    for label, count in stats['spacy_errors'].items():\n",
        "        print(f\"{label}: {count} errors\")\n",
        "\n",
        "    print(\"\\n\\033[1;34mCorrections by Entity Type:\\033[0m\")\n",
        "    for label, count in stats['corrections'].items():\n",
        "        print(f\"{label}: {count} corrections\")\n",
        "\n",
        "    print(\"\\n\\033[1;34mImprovement Source:\\033[0m\")\n",
        "    print(\"\\033[1;36mRegex Patterns:\\033[0m\")\n",
        "    for label, count in stats['regex_improvements'].items():\n",
        "        print(f\"{label}: {count} improvements\")\n",
        "\n",
        "    print(\"\\n\\033[1;35mLearned Patterns:\\033[0m\")\n",
        "    for label, count in stats['learned_improvements'].items():\n",
        "        print(f\"{label}: {count} improvements\")\n"
      ],
      "metadata": {
        "id": "4vhIRae2SqGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Standard evaluation function\n",
        "# ------------------------------\n",
        "def evaluate_ner_system(dataset, ner_system):\n",
        "    \"\"\"Standard evaluation using seqeval\"\"\"\n",
        "    # Process test data\n",
        "    spacy_predictions = []\n",
        "    enhanced_predictions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for example in dataset[\"test\"]:\n",
        "        tokens = example[\"tokens\"]\n",
        "        predictions = ner_system.extract_entities_with_analysis(tokens)\n",
        "\n",
        "        # Convert ground truth tags to BIO format\n",
        "        ner_labels = dataset[\"train\"].features[\"ner_tags\"].feature\n",
        "        ground_truth = [ner_labels.int2str(tag) for tag in example[\"ner_tags\"]]\n",
        "        filtered_truth = [tag if any(e in tag for e in ner_system.allowed_entities) else \"O\" for tag in ground_truth]\n",
        "\n",
        "        spacy_predictions.append(predictions[\"spacy\"])\n",
        "        enhanced_predictions.append(predictions[\"corrected\"])\n",
        "        ground_truths.append(filtered_truth)\n",
        "\n",
        "    # Evaluate results\n",
        "    metric = load(\"seqeval\")\n",
        "\n",
        "    spacy_results = metric.compute(predictions=spacy_predictions, references=ground_truths)\n",
        "    enhanced_results = metric.compute(predictions=enhanced_predictions, references=ground_truths)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\\033[1;34mBaseline spaCy Results:\\033[0m\")\n",
        "    spacy_table = []\n",
        "    for entity, metrics in spacy_results.items():\n",
        "        if isinstance(metrics, dict) and entity in ner_system.allowed_entities:\n",
        "            spacy_table.append([entity, metrics[\"precision\"], metrics[\"recall\"], metrics[\"f1\"], metrics[\"number\"]])\n",
        "    print(tabulate(spacy_table, headers=[\"Entity\", \"Precision\", \"Recall\", \"F1 Score\", \"Count\"], tablefmt=\"pretty\"))\n",
        "    print(f\"Overall spaCy F1: {spacy_results['overall_f1']:.4f}\")\n",
        "\n",
        "    print(\"\\n\\033[1;34mEnhanced spaCy NER Results:\\033[0m\")\n",
        "    enhanced_table = []\n",
        "    for entity, metrics in enhanced_results.items():\n",
        "        if isinstance(metrics, dict) and entity in ner_system.allowed_entities:\n",
        "            enhanced_table.append([entity, metrics[\"precision\"], metrics[\"recall\"], metrics[\"f1\"], metrics[\"number\"]])\n",
        "    print(tabulate(enhanced_table, headers=[\"Entity\", \"Precision\", \"Recall\", \"F1 Score\", \"Count\"], tablefmt=\"pretty\"))\n",
        "    print(f\"Overall Enhanced F1: {enhanced_results['overall_f1']:.4f}\")\n",
        "\n",
        "    # Show improvements\n",
        "    print(\"\\n\\033[1;34mImprovement Analysis:\\033[0m\")\n",
        "    for entity in ner_system.allowed_entities:\n",
        "        if entity in spacy_results and entity in enhanced_results:\n",
        "            f1_improvement = enhanced_results[entity][\"f1\"] - spacy_results[entity][\"f1\"]\n",
        "            precision_improvement = enhanced_results[entity][\"precision\"] - spacy_results[entity][\"precision\"]\n",
        "            recall_improvement = enhanced_results[entity][\"recall\"] - spacy_results[entity][\"recall\"]\n",
        "\n",
        "            print(f\"{entity}: F1 Δ = {f1_improvement:+.4f}, Precision Δ = {precision_improvement:+.4f}, Recall Δ = {recall_improvement:+.4f}\")\n",
        "\n",
        "    overall_f1_improvement = enhanced_results['overall_f1'] - spacy_results['overall_f1']\n",
        "    print(f\"\\nOverall F1 Improvement: {overall_f1_improvement:+.4f}\")\n"
      ],
      "metadata": {
        "id": "S8wxsUG4Sy5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Main execution function\n",
        "# -------------------------\n",
        "def run_full_analysis():\n",
        "    \"\"\"Run full analysis with examples and statistics\"\"\"\n",
        "    # Load dataset\n",
        "    print(\"Loading CoNLL2003 dataset...\")\n",
        "    dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "    # Initialize NER system\n",
        "    print(\"Initializing Enhanced NER system...\")\n",
        "    ner_system = EnhancedSpacyNER()\n",
        "\n",
        "    print(\"Learning correction patterns from training data...\")\n",
        "    ner_system.load_correction_patterns_from_training(dataset)\n",
        "\n",
        "    # Run analysis\n",
        "    print(\"\\nAnalyzing examples where corrections were made...\\n\")\n",
        "    test_results = analyze_examples(dataset, ner_system, max_examples=30)\n",
        "\n",
        "    # Generate statistics\n",
        "    print(\"\\nGenerating statistics...\")\n",
        "    stats = generate_statistics(test_results)\n",
        "\n",
        "    # Display analytics\n",
        "    display_analytics(stats)\n",
        "\n",
        "    # Run standard evaluation\n",
        "    print(\"\\n\\nRunning standard evaluation...\")\n",
        "    evaluate_ner_system(dataset, ner_system)"
      ],
      "metadata": {
        "id": "swxxay_xSsvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Helper function for sample analysis\n",
        "# ------------------------------------\n",
        "def analyze_sample(ner_system, text):\n",
        "    \"\"\"Analyze a single sample text\"\"\"\n",
        "    tokens = text.split()\n",
        "    predictions = ner_system.extract_entities_with_analysis(tokens)\n",
        "\n",
        "    print(\"\\n\\033[1;34mToken Analysis:\\033[0m\")\n",
        "    print(\"\\033[1;33mTokens:     \\033[0m\", \" \".join(tokens))\n",
        "    print(\"\\033[1;31mSpaCy:      \\033[0m\", \" \".join(predictions[\"spacy\"]))\n",
        "    print(\"\\033[1;36mRegex:      \\033[0m\", \" \".join(predictions[\"regex\"]))\n",
        "    print(\"\\033[1;35mLearned:    \\033[0m\", \" \".join(predictions[\"custom\"]))\n",
        "    print(\"\\033[1;36mCorrected:  \\033[0m\", \" \".join(predictions[\"corrected\"]))\n",
        "\n",
        "    # Identify differences\n",
        "    print(\"\\n\\033[1;34mDifferences:\\033[0m\")\n",
        "    for i, token in enumerate(tokens):\n",
        "        if predictions[\"spacy\"][i] != predictions[\"corrected\"][i]:\n",
        "            print(f\"{token}: SpaCy={predictions['spacy'][i]}, Final={predictions['corrected'][i]}\")\n"
      ],
      "metadata": {
        "id": "YQxRH_zDSv_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the complete analysis\n",
        "if __name__ == \"__main__\":\n",
        "    run_full_analysis()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9ONfdNwS2bm",
        "outputId": "229fa7a8-1e20-47c8-c1b7-c4054148a7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CoNLL2003 dataset...\n",
            "Initializing Enhanced NER system...\n",
            "Learning correction patterns from training data...\n",
            "\n",
            "Analyzing examples where corrections were made...\n",
            "\n",
            "\n",
            "\u001b[1;34mExample 1:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O O B-LOC O O O O B-PER O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O O O O O O O O O B-ORG O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O O O O O O O O O B-ORG O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O O O O O O O O O B-ORG O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "JAPAN: GT=B-LOC, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "CHINA: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "DEFEAT: GT=O, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 3:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m AL-AIN , United Arab Emirates 1996-12-06\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-LOC O B-LOC I-LOC I-LOC O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-ORG O B-LOC I-LOC I-LOC O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-ORG O B-LOC I-LOC I-LOC O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-ORG O B-LOC I-LOC I-LOC O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "AL-AIN: GT=B-LOC, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 4:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-LOC O O O O O B-MISC I-MISC O O O O O O O B-LOC O O O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-LOC O O O O O B-MISC I-MISC O O O O O O O B-LOC O O B-ORG I-ORG O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O B-MISC I-MISC O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-LOC O O O O O B-MISC B-MISC O O O O O O O B-LOC O O B-ORG I-ORG O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-LOC O O O O O B-MISC I-MISC O O O O O O O B-LOC O O B-ORG I-ORG O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Group: GT=O, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "C: GT=O, SpaCy=I-ORG, Regex=O, Learned=I-ORG, Final=I-ORG\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 5:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O B-LOC O O O O O O O O O O O O O O O O O O O O O B-LOC O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O B-LOC O O O O O O O O O O O O O O O O O O O O O B-ORG O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O B-LOC O O O O O O O O O O O O O O O O O O O O O B-ORG O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O B-LOC O O O O O O O O O O O O O O O O O O O O O B-ORG O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Uzbekistan: GT=B-LOC, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 7:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Oleg Shatskiku made sure of the win in injury time , hitting an unstoppable left foot shot from just outside the area .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-PER I-PER O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Oleg: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Shatskiku: GT=I-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 8:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m The former Soviet republic was playing in an Asian Cup finals tie for the first time .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O O B-MISC O O O O O B-MISC I-MISC O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O B-MISC O O O O O O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O B-MISC I-MISC O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O B-MISC O O O O O O B-MISC O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O B-MISC O O O O O B-MISC B-MISC O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Asian: GT=B-MISC, SpaCy=O, Regex=B-MISC, Learned=O, Final=B-MISC\n",
            "Cup: GT=I-MISC, SpaCy=O, Regex=I-MISC, Learned=B-MISC, Final=B-MISC\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 9:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Despite winning the Asian Games title two years ago , Uzbekistan are in the finals as outsiders .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O O O B-MISC I-MISC O O O O O B-LOC O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O B-MISC I-MISC I-MISC O O O O O B-MISC O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O B-MISC I-MISC O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O B-MISC I-MISC I-MISC O O O O O B-MISC O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O B-MISC I-MISC I-MISC O O O O O B-MISC O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "the: GT=O, SpaCy=B-MISC, Regex=O, Learned=B-MISC, Final=B-MISC\n",
            "Asian: GT=B-MISC, SpaCy=I-MISC, Regex=B-MISC, Learned=I-MISC, Final=I-MISC\n",
            "Uzbekistan: GT=B-LOC, SpaCy=B-MISC, Regex=O, Learned=B-MISC, Final=B-MISC\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 11:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Takuya Takagi scored the winner in the 88th minute , rising to head a Hiroshige Yanagimoto cross towards the Syrian goal which goalkeeper Salem Bitar appeared to have covered but then allowed to slip into the net .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-PER I-PER O O O O O O O O O O O O B-PER I-PER O O O B-MISC O O O B-PER I-PER O O O O O O O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-PER I-PER O O O O O O O O O O O O B-PER I-PER O O O B-MISC O O O B-ORG I-ORG O O O O O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-PER I-PER O O O O O O O O O O O O B-PER I-PER O O O B-MISC O O O B-ORG I-ORG O O O O O O O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-PER I-PER O O O O O O O O O O O O B-PER I-PER O O O B-MISC O O O B-ORG I-ORG O O O O O O O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Salem: GT=B-PER, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "Bitar: GT=I-PER, SpaCy=I-ORG, Regex=O, Learned=I-ORG, Final=I-ORG\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 13:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Defender Hassan Abbas rose to intercept a long ball into the area in the 84th minute but only managed to divert it into the top corner of Bitar 's goal .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Defender: GT=O, SpaCy=B-PER, Regex=O, Learned=B-PER, Final=B-PER\n",
            "Hassan: GT=B-PER, SpaCy=I-PER, Regex=O, Learned=I-PER, Final=I-PER\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 15:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Japan then laid siege to the Syrian penalty area for most of the game but rarely breached the Syrian defence .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-LOC O O O O O B-MISC O O O O O O O O O O O B-MISC O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-LOC O O O O O B-MISC O O O O O O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-LOC O O O O O B-MISC O O O O O O O O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-LOC O O O O O B-MISC O O O O O O O O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Syrian: GT=B-MISC, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 20:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Japan , co-hosts of the World Cup in 2002 and ranked 20th in the world by FIFA , are favourites to regain their title here .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-LOC O O O O B-MISC I-MISC O O O O O O O O O B-ORG O O O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-LOC O O O B-MISC I-MISC I-MISC O O O O O O O O O B-ORG O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O B-MISC I-MISC O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-LOC O O O B-MISC I-MISC B-MISC O O O O O O O O O B-ORG O O O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-LOC O O O B-MISC I-MISC I-MISC O O O O O O O O O B-ORG O O O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "the: GT=O, SpaCy=B-MISC, Regex=O, Learned=B-MISC, Final=B-MISC\n",
            "World: GT=B-MISC, SpaCy=I-MISC, Regex=B-MISC, Learned=I-MISC, Final=I-MISC\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 21:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Hosts UAE play Kuwait and South Korea take on Indonesia on Saturday in Group A matches .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O B-LOC O B-LOC O B-LOC I-LOC O O B-LOC O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O O B-LOC O B-LOC I-LOC O O B-LOC O O O B-ORG O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O O B-LOC O B-LOC I-LOC O O B-LOC O O O B-ORG O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O O B-LOC O B-LOC I-LOC O O B-LOC O O O B-ORG O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "UAE: GT=B-LOC, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Group: GT=O, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 23:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m RUGBY UNION - CUTTITTA BACK FOR ITALY AFTER A YEAR .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-ORG I-ORG O B-PER O O B-LOC O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-ORG I-ORG O O O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-ORG I-ORG O O O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "RUGBY: GT=B-ORG, SpaCy=O, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "UNION: GT=I-ORG, SpaCy=O, Regex=O, Learned=I-ORG, Final=I-ORG\n",
            "CUTTITTA: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "ITALY: GT=B-LOC, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 25:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Italy recalled Marcello Cuttitta\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-LOC O B-PER I-PER\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-LOC O B-ORG B-MISC\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-LOC O B-ORG B-MISC\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-LOC O B-ORG B-MISC\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Marcello: GT=B-PER, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "Cuttitta: GT=I-PER, SpaCy=B-MISC, Regex=O, Learned=B-MISC, Final=B-MISC\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 26:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m on Friday for their friendly against Scotland at Murrayfield more than a year after the 30-year-old wing announced he was retiring following differences over selection .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O O O O O O B-LOC O B-LOC O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O O O O O B-LOC O B-ORG O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O O O O O B-LOC O B-ORG O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O O O O O B-LOC O B-ORG O O O O O O O O O O O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Murrayfield: GT=B-LOC, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 27:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Cuttitta , who trainer George Coste said was certain to play on Saturday week , was named in a 21-man squad lacking only two of the team beaten 54-21 by England at Twickenham last month .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-PER O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-MISC O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-MISC O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-MISC O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-LOC O B-LOC O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Cuttitta: GT=B-PER, SpaCy=B-MISC, Regex=O, Learned=B-MISC, Final=B-MISC\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 28:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Stefano Bordon is out through illness and Coste said he had dropped back row Corrado Covi , who had been recalled for the England game after five years out of the national team .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-PER I-PER O O O O O B-PER O O O O O O B-PER I-PER O O O O O O O B-LOC O O O O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-ORG I-ORG O O O O O B-PER O O O O O O B-PER I-PER O O O O O O O B-LOC O O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-ORG I-ORG O O O O O B-PER O O O O O O B-PER I-PER O O O O O O O B-LOC O O O O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-ORG I-ORG O O O O O B-PER O O O O O O B-PER I-PER O O O O O O O B-LOC O O O O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Stefano: GT=B-PER, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "Bordon: GT=I-PER, SpaCy=I-ORG, Regex=O, Learned=I-ORG, Final=I-ORG\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 29:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Cuttitta announced his retirement after the 1995 World Cup , where he took issue with being dropped from the Italy side that faced England in the pool stages .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-PER O O O O O B-MISC I-MISC I-MISC O O O O O O O O O O B-LOC O O O B-LOC O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-ORG O O O O B-MISC I-MISC B-MISC I-MISC O O O O O O O O O O B-LOC O O O B-LOC O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O B-MISC I-MISC O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-ORG O O O O B-MISC I-MISC B-MISC B-MISC O O O O O O O O O O B-LOC O O O B-LOC O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-ORG O O O O B-MISC I-MISC B-MISC I-MISC O O O O O O O O O O B-LOC O O O B-LOC O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Cuttitta: GT=B-PER, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "the: GT=O, SpaCy=B-MISC, Regex=O, Learned=B-MISC, Final=B-MISC\n",
            "1995: GT=B-MISC, SpaCy=I-MISC, Regex=O, Learned=I-MISC, Final=I-MISC\n",
            "World: GT=I-MISC, SpaCy=B-MISC, Regex=B-MISC, Learned=B-MISC, Final=B-MISC\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 30:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Coste said he had approached the player two months ago about a comeback .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-PER O O O O O O O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O O O O O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O O O O O O O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O O O O O O O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Coste: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 31:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m \" He ended the World Cup on the wrong note , \" Coste said .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O O O O B-MISC I-MISC O O O O O O B-PER O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O O B-MISC I-MISC I-MISC O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O B-MISC I-MISC O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O O B-MISC I-MISC B-MISC O O O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O O B-MISC I-MISC I-MISC O O O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "the: GT=O, SpaCy=B-MISC, Regex=O, Learned=B-MISC, Final=B-MISC\n",
            "World: GT=B-MISC, SpaCy=I-MISC, Regex=B-MISC, Learned=I-MISC, Final=I-MISC\n",
            "Coste: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 34:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Squad : Javier Pertile , Paolo Vaccari , Marcello Cuttitta , Ivan Francescato , Leandro Manteri , Diego Dominguez , Francesco Mazzariol , Alessandro Troncon , Orazio Arancio , Andrea Sgorlon , Massimo Giovanelli , Carlo Checchinato , Walter Cristofoletto , Franco Properzi Curti , Carlo Orlandi , Massimo Cuttitta , Giambatista Croci , Gianluca Guidi , Nicola Mazzucato , Alessandro Moscardi , Andrea Castellani .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-ORG I-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-ORG I-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-ORG I-ORG O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O B-PER I-PER O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Alessandro: GT=B-PER, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "Troncon: GT=I-PER, SpaCy=I-ORG, Regex=O, Learned=I-ORG, Final=I-ORG\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 35:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m SOCCER - LATE GOALS GIVE JAPAN WIN OVER SYRIA .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O O O O O B-LOC O O B-LOC O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O O O O O O O B-LOC O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O O O O O O O B-LOC O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O O O O O O O B-LOC O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "JAPAN: GT=B-LOC, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 36:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m AL-AIN , United Arab Emirates 1996-12-06\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-LOC O B-LOC I-LOC I-LOC O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-ORG O B-LOC I-LOC I-LOC O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-ORG O B-LOC I-LOC I-LOC O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-ORG O B-LOC I-LOC I-LOC O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "AL-AIN: GT=B-LOC, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 37:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Two goals in the last six minutes gave holders Japan an uninspiring 2-1 Asian Cup victory over Syria on Friday .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O O O O O O O O O B-LOC O O O B-MISC I-MISC O O B-LOC O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O O O O O O O O B-LOC O O O O O O O B-LOC O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O B-MISC I-MISC O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O O O O O O O O B-LOC O O O O B-MISC O O B-LOC O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O O O O O O O O B-LOC O O O B-MISC B-MISC O O B-LOC O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Asian: GT=B-MISC, SpaCy=O, Regex=B-MISC, Learned=O, Final=B-MISC\n",
            "Cup: GT=I-MISC, SpaCy=O, Regex=I-MISC, Learned=B-MISC, Final=B-MISC\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 40:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Defender Hassan Abbas rose to intercept a long ball into the area in the 84th minute but only managed to divert it into the top corner of Bitar 's goal .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-PER I-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O B-PER O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Defender: GT=O, SpaCy=B-PER, Regex=O, Learned=B-PER, Final=B-PER\n",
            "Hassan: GT=B-PER, SpaCy=I-PER, Regex=O, Learned=I-PER, Final=I-PER\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 42:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Nader Jokhadar headed a cross from the right by Ammar Awad into the top right corner of Kenichi Shimokawa 's goal .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-PER I-PER O O O O O O O B-PER I-PER O O O O O O B-PER I-PER O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-PER I-PER O O O O O O O B-PER I-PER O O O O O O B-LOC I-LOC I-LOC O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-PER I-PER O O O O O O O B-PER I-PER O O O O O O B-LOC I-LOC I-LOC O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-PER I-PER O O O O O O O B-PER I-PER O O O O O O B-LOC I-LOC I-LOC O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Kenichi: GT=B-PER, SpaCy=B-LOC, Regex=O, Learned=B-LOC, Final=B-LOC\n",
            "Shimokawa: GT=I-PER, SpaCy=I-LOC, Regex=O, Learned=I-LOC, Final=I-LOC\n",
            "'s: GT=O, SpaCy=I-LOC, Regex=O, Learned=I-LOC, Final=I-LOC\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 44:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m A minute later , Bitar produced a good double save , first from Kazuyoshi Miura 's header and then blocked a Takagi follow-up shot .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m O O O O B-PER O O O O O O O O B-PER I-PER O O O O O O B-PER O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m O O O O B-PER O O O O O O O O B-PER I-PER I-PER O O O O O B-MISC O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m O O O O B-PER O O O O O O O O B-PER I-PER I-PER O O O O O B-MISC O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m O O O O B-PER O O O O O O O O B-PER I-PER I-PER O O O O O B-MISC O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "'s: GT=O, SpaCy=I-PER, Regex=O, Learned=I-PER, Final=I-PER\n",
            "Takagi: GT=B-PER, SpaCy=B-MISC, Regex=O, Learned=B-MISC, Final=B-MISC\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 45:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Bitar saved well again from Miura in the 37th minute , parrying away his header from a corner .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-PER O O O O B-PER O O O O O O O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-PER O O O O B-LOC O O O O O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-PER O O O O B-LOC O O O O O O O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-PER O O O O B-LOC O O O O O O O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Miura: GT=B-PER, SpaCy=B-LOC, Regex=O, Learned=B-LOC, Final=B-LOC\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 46:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Japan started the second half brightly but Bitar denied them an equaliser when he dived to his right to save Naoki Soma 's low drive in the 53rd minute .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-LOC O O O O O O B-PER O O O O O O O O O O O O B-PER I-PER O O O O O O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-LOC O O O O O O B-PER O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-LOC O O O O O O B-PER O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-LOC O O O O O O B-PER O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Naoki: GT=B-PER, SpaCy=B-ORG, Regex=O, Learned=B-ORG, Final=B-ORG\n",
            "Soma: GT=I-PER, SpaCy=I-ORG, Regex=O, Learned=I-ORG, Final=I-ORG\n",
            "'s: GT=O, SpaCy=I-ORG, Regex=O, Learned=I-ORG, Final=I-ORG\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1;34mExample 47:\u001b[0m\n",
            "\u001b[1;33mTokens: \u001b[0m Japan : 19 - Kenichi Shimokawa , 2 - Hiroshige Yanagimoto , 3 - Naoki Soma , 4 - Masami Ihara , 5 - Norio Omura , 6 - Motohiro Yamaguchi , 8 - Masakiyo Maezono ( 7 - Yasuto Honda 71 ) , 9 - Takuya Takagi , 10 - Hiroshi Nanami , 11 - Kazuyoshi Miura , 15 - Hiroaki Morishima ( 14 - Masayuki Okano 75 ) .\n",
            "\u001b[1;32mGround Truth:    \u001b[0m B-LOC O O O B-PER I-PER O O O B-PER I-PER O O O B-PER I-PER O O O B-PER I-PER O O O B-PER I-PER O O O B-PER I-PER O O O B-PER I-PER O O O B-PER I-PER O O O O O B-PER I-PER O O O B-PER I-PER O O O B-PER I-PER O O O B-PER I-PER O O O B-PER I-PER O O O\n",
            "\u001b[1;31mSpaCy Prediction: \u001b[0m B-LOC O O O O O O O O B-PER I-PER O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O B-PER I-PER O O O O O O O O O O O O O\n",
            "\u001b[1;36mRegex Patterns:   \u001b[0m O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\u001b[1;35mLearned Patterns: \u001b[0m B-LOC O O O O O O O O B-PER I-PER O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O B-PER I-PER O O O O O O O O O O O O O\n",
            "\u001b[1;36mFinal Corrected:  \u001b[0m B-LOC O O O O O O O O B-PER I-PER O O O O O O O O B-PER I-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-PER I-PER O O O B-PER I-PER O O O O O O O O O O O O O\n",
            "\n",
            "\u001b[1;35mMismatched Entities:\u001b[0m\n",
            "Kenichi: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Shimokawa: GT=I-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Naoki: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Soma: GT=I-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Norio: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Omura: GT=I-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Motohiro: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Yamaguchi: GT=I-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Masakiyo: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Maezono: GT=I-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Yasuto: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Honda: GT=I-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Takuya: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Takagi: GT=I-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Hiroaki: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Morishima: GT=I-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Masayuki: GT=B-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "Okano: GT=I-PER, SpaCy=O, Regex=O, Learned=O, Final=O\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Generating statistics...\n",
            "\n",
            "\u001b[1;34mAnalysis Summary:\u001b[0m\n",
            "Total tokens analyzed: 46435\n",
            "Total corrections made: 313\n",
            "Correction rate: 0.67%\n",
            "\n",
            "\u001b[1;34mError Distribution (spaCy):\u001b[0m\n",
            "B-LOC: 453 errors\n",
            "B-PER: 580 errors\n",
            "O: 814 errors\n",
            "I-PER: 264 errors\n",
            "B-MISC: 316 errors\n",
            "I-MISC: 137 errors\n",
            "B-ORG: 1067 errors\n",
            "I-ORG: 377 errors\n",
            "I-LOC: 97 errors\n",
            "\n",
            "\u001b[1;34mCorrections by Entity Type:\u001b[0m\n",
            "B-MISC: 60 corrections\n",
            "B-ORG: 76 corrections\n",
            "I-ORG: 55 corrections\n",
            "B-LOC: 77 corrections\n",
            "I-LOC: 11 corrections\n",
            "B-PER: 7 corrections\n",
            "I-PER: 6 corrections\n",
            "I-MISC: 21 corrections\n",
            "\n",
            "\u001b[1;34mImprovement Source:\u001b[0m\n",
            "\u001b[1;36mRegex Patterns:\u001b[0m\n",
            "O: 811 improvements\n",
            "B-MISC: 24 improvements\n",
            "I-MISC: 17 improvements\n",
            "B-LOC: 4 improvements\n",
            "I-LOC: 2 improvements\n",
            "B-ORG: 8 improvements\n",
            "I-ORG: 3 improvements\n",
            "\n",
            "\u001b[1;35mLearned Patterns:\u001b[0m\n",
            "B-ORG: 78 improvements\n",
            "I-ORG: 56 improvements\n",
            "B-LOC: 76 improvements\n",
            "I-LOC: 10 improvements\n",
            "B-MISC: 48 improvements\n",
            "B-PER: 7 improvements\n",
            "I-PER: 6 improvements\n",
            "I-MISC: 17 improvements\n",
            "\n",
            "\n",
            "Running standard evaluation...\n",
            "\n",
            "\u001b[1;34mBaseline spaCy Results:\u001b[0m\n",
            "+--------+---------------------+--------------------+--------------------+-------+\n",
            "| Entity |      Precision      |       Recall       |      F1 Score      | Count |\n",
            "+--------+---------------------+--------------------+--------------------+-------+\n",
            "|  LOC   | 0.7849673202614379  | 0.7200239808153477 | 0.7510944340212632 | 1668  |\n",
            "|  MISC  | 0.7140186915887851  | 0.5441595441595442 | 0.6176232821341956 |  702  |\n",
            "|  ORG   | 0.44278169014084506 | 0.3028296207104154 | 0.3596710761530211 | 1661  |\n",
            "|  PER   | 0.7384500745156483  | 0.6128633271490415 | 0.6698208854342683 | 1617  |\n",
            "+--------+---------------------+--------------------+--------------------+-------+\n",
            "Overall spaCy F1: 0.6039\n",
            "\n",
            "\u001b[1;34mEnhanced spaCy NER Results:\u001b[0m\n",
            "+--------+---------------------+---------------------+--------------------+-------+\n",
            "| Entity |      Precision      |       Recall        |      F1 Score      | Count |\n",
            "+--------+---------------------+---------------------+--------------------+-------+\n",
            "|  LOC   | 0.7699386503067485  | 0.7523980815347722  | 0.7610673135233474 | 1668  |\n",
            "|  MISC  | 0.6810207336523126  | 0.6082621082621082  | 0.6425884123401053 |  702  |\n",
            "|  ORG   | 0.45737704918032784 | 0.33594220349187237 | 0.3873654980909407 | 1661  |\n",
            "|  PER   | 0.7414561664190193  | 0.6171923314780458  | 0.6736415794802566 | 1617  |\n",
            "+--------+---------------------+---------------------+--------------------+-------+\n",
            "Overall Enhanced F1: 0.6185\n",
            "\n",
            "\u001b[1;34mImprovement Analysis:\u001b[0m\n",
            "LOC: F1 Δ = +0.0100, Precision Δ = -0.0150, Recall Δ = +0.0324\n",
            "MISC: F1 Δ = +0.0250, Precision Δ = -0.0330, Recall Δ = +0.0641\n",
            "ORG: F1 Δ = +0.0277, Precision Δ = +0.0146, Recall Δ = +0.0331\n",
            "PER: F1 Δ = +0.0038, Precision Δ = +0.0030, Recall Δ = +0.0043\n",
            "\n",
            "Overall F1 Improvement: +0.0146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_enhanced_ner_system(dataset, ner_system):\n",
        "    \"\"\"Evaluate enhanced NER system using seqeval\"\"\"\n",
        "    # Process test data\n",
        "    enhanced_predictions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for example in dataset[\"test\"]:\n",
        "        tokens = example[\"tokens\"]\n",
        "\n",
        "        # Get enhanced predictions only\n",
        "        predictions = ner_system.extract_entities_with_analysis(tokens)\n",
        "\n",
        "        # Convert ground truth tags to BIO format\n",
        "        ner_labels = dataset[\"train\"].features[\"ner_tags\"].feature\n",
        "        ground_truth = [ner_labels.int2str(tag) for tag in example[\"ner_tags\"]]\n",
        "        filtered_truth = [tag if any(e in tag for e in ner_system.allowed_entities) else \"O\" for tag in ground_truth]\n",
        "\n",
        "        enhanced_predictions.append(predictions[\"corrected\"])\n",
        "        ground_truths.append(filtered_truth)\n",
        "\n",
        "    # Evaluate using seqeval\n",
        "    metric = load(\"seqeval\")\n",
        "    enhanced_results = metric.compute(predictions=enhanced_predictions, references=ground_truths)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\\033[1;34mEnhanced SpaCy NER Evaluation Results:\\033[0m\")\n",
        "\n",
        "    # Create detailed table for per-entity results\n",
        "    enhanced_table = []\n",
        "    for entity, metrics in enhanced_results.items():\n",
        "        if isinstance(metrics, dict) and entity in ner_system.allowed_entities:\n",
        "            enhanced_table.append([\n",
        "                entity,\n",
        "                f\"{metrics['precision']:.4f}\",\n",
        "                f\"{metrics['recall']:.4f}\",\n",
        "                f\"{metrics['f1']:.4f}\",\n",
        "                metrics['number']\n",
        "            ])\n",
        "\n",
        "    print(tabulate(enhanced_table, headers=[\"Entity\", \"Precision\", \"Recall\", \"F1 Score\", \"Support\"], tablefmt=\"pretty\"))\n",
        "\n",
        "    # Overall results\n",
        "    print(f\"\\n\\033[1;36mOverall Metrics:\\033[0m\")\n",
        "    print(f\"Overall Precision: {enhanced_results['overall_precision']:.4f}\")\n",
        "    print(f\"Overall Recall: {enhanced_results['overall_recall']:.4f}\")\n",
        "    print(f\"Overall F1 Score: {enhanced_results['overall_f1']:.4f}\")\n",
        "    print(f\"Overall Accuracy: {enhanced_results['overall_accuracy']:.4f}\")\n",
        "\n",
        "    return enhanced_results\n",
        "\n",
        "# Generate classification report using seqeval format\n",
        "# ------------------------------------------------\n",
        "def generate_seqeval_report(dataset, ner_system):\n",
        "    \"\"\"Generate detailed seqeval classification report\"\"\"\n",
        "    # Process test data\n",
        "    enhanced_predictions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for example in dataset[\"test\"]:\n",
        "        tokens = example[\"tokens\"]\n",
        "        predictions = ner_system.extract_entities_with_analysis(tokens)\n",
        "\n",
        "        # Convert ground truth tags to BIO format\n",
        "        ner_labels = dataset[\"train\"].features[\"ner_tags\"].feature\n",
        "        ground_truth = [ner_labels.int2str(tag) for tag in example[\"ner_tags\"]]\n",
        "        filtered_truth = [tag if any(e in tag for e in ner_system.allowed_entities) else \"O\" for tag in ground_truth]\n",
        "\n",
        "        enhanced_predictions.append(predictions[\"corrected\"])\n",
        "        ground_truths.append(filtered_truth)\n",
        "\n",
        "    # Generate seqeval classification report\n",
        "    metric = load(\"seqeval\")\n",
        "    print(\"\\n\\033[1;34mEnhanced NER Classification Report (seqeval):\\033[0m\")\n",
        "    print(metric.compute(predictions=enhanced_predictions, references=ground_truths, mode='strict', scheme='IOB2'))\n",
        "\n",
        "    return enhanced_predictions, ground_truths\n",
        "\n",
        "# Main execution function for enhanced evaluation only\n",
        "# -------------------------------------------------\n",
        "def run_enhanced_evaluation():\n",
        "    \"\"\"Run evaluation focusing only on enhanced NER system\"\"\"\n",
        "    # Load dataset\n",
        "    print(\"Loading CoNLL2003 dataset...\")\n",
        "    dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "    # Initialize NER system\n",
        "    print(\"Initializing Enhanced NER system...\")\n",
        "    ner_system = EnhancedSpacyNER()\n",
        "\n",
        "    print(\"Learning correction patterns from training data...\")\n",
        "    ner_system.load_correction_patterns_from_training(dataset)\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"\\nEvaluating Enhanced NER system...\")\n",
        "    enhanced_results = evaluate_enhanced_ner_system(dataset, ner_system)\n",
        "\n",
        "    # Generate detailed classification report\n",
        "    print(\"\\nGenerating detailed classification report...\")\n",
        "    generate_seqeval_report(dataset, ner_system)\n",
        "\n",
        "    return enhanced_results\n",
        "\n",
        "# Run the enhanced evaluation\n",
        "if __name__ == \"__main__\":\n",
        "    run_enhanced_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFnIpVZ4VxHF",
        "outputId": "dc89d42d-36b7-4e9c-97ed-a8832d29524f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CoNLL2003 dataset...\n",
            "Initializing Enhanced NER system...\n",
            "Learning correction patterns from training data...\n",
            "\n",
            "Evaluating Enhanced NER system...\n",
            "\n",
            "\u001b[1;34mEnhanced SpaCy NER Evaluation Results:\u001b[0m\n",
            "+--------+-----------+--------+----------+---------+\n",
            "| Entity | Precision | Recall | F1 Score | Support |\n",
            "+--------+-----------+--------+----------+---------+\n",
            "|  LOC   |  0.7699   | 0.7524 |  0.7611  |  1668   |\n",
            "|  MISC  |  0.6810   | 0.6083 |  0.6426  |   702   |\n",
            "|  ORG   |  0.4574   | 0.3359 |  0.3874  |  1661   |\n",
            "|  PER   |  0.7415   | 0.6172 |  0.6736  |  1617   |\n",
            "+--------+-----------+--------+----------+---------+\n",
            "\n",
            "\u001b[1;36mOverall Metrics:\u001b[0m\n",
            "Overall Precision: 0.6714\n",
            "Overall Recall: 0.5733\n",
            "Overall F1 Score: 0.6185\n",
            "Overall Accuracy: 0.9164\n",
            "\n",
            "Generating detailed classification report...\n",
            "\n",
            "\u001b[1;34mEnhanced NER Classification Report (seqeval):\u001b[0m\n",
            "{'LOC': {'precision': np.float64(0.7713583282114321), 'recall': np.float64(0.7523980815347722), 'f1': np.float64(0.7617602427921093), 'number': np.int64(1668)}, 'MISC': {'precision': np.float64(0.6864951768488746), 'recall': np.float64(0.6082621082621082), 'f1': np.float64(0.6450151057401812), 'number': np.int64(702)}, 'ORG': {'precision': np.float64(0.4644945697577276), 'recall': np.float64(0.3347381095725467), 'f1': np.float64(0.38908327501749473), 'number': np.int64(1661)}, 'PER': {'precision': np.float64(0.7462574850299402), 'recall': np.float64(0.616573902288188), 'f1': np.float64(0.675245513037589), 'number': np.int64(1617)}, 'overall_precision': np.float64(0.6764951902969469), 'overall_recall': np.float64(0.5727691218130312), 'overall_f1': np.float64(0.6203259827420902), 'overall_accuracy': 0.9163561968342845}\n"
          ]
        }
      ]
    }
  ]
}